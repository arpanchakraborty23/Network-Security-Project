{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOPS\\\\NetWork Security\\\\expariments'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOPS\\\\NetWork Security'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Pipline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networksecurity.constant import traning_pipline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class TraningPiplineConfig:\n",
    "    def __init__(self) -> None:\n",
    "      self.pipline_name=traning_pipline.PIPELINE_NAME   \n",
    "      self.artifact_name=traning_pipline.ARTIFACT_DIR\n",
    "      self.artifact_dir=os.path.join(self.artifact_name)\n",
    "      \n",
    "class DataValidationConfig:\n",
    "    def __init__(self,traning_pipline_config:TraningPiplineConfig) -> None:\n",
    "      self.data_validation_dir:str=os.path.join(\n",
    "      \ttraning_pipline_config.artifact_dir,traning_pipline.DATA_VALIDATION_DIR_NAME ## crating data validaton folder inside artifacts\n",
    "\t\t)\n",
    "      self.valid_dir_name:str=os.path.join(\n",
    "         self.data_validation_dir, traning_pipline.DATA_VALIDATION_VALID_DIR ## validated report folder inside data validation folder\n",
    "\t\t)\n",
    "      self.invalid_dir_name:str=os.path.join(\n",
    "         self.data_validation_dir,traning_pipline.DATA_VALIDATION_INVALID_DIR ## invalid report folder inside data validation folder\n",
    "\t\t)\n",
    "      self.drift_report_dir:str=os.path.join(\n",
    "         self.data_validation_dir,traning_pipline.DATA_VALIDATION_DRIFT_REPORT_DIR,traning_pipline.DATA_VALIDATION_DRIFT_REPORT_FILE_NAME # data validation dir, drift report dir, report name\n",
    "\t\t)\n",
    "      self.valid_traning_data_store_path:str=os.path.join(\n",
    "         traning_pipline_config.artifact_dir, traning_pipline.DATA_INGESTION__DIR, traning_pipline.TRAIN_FILE_NAME  ## artifacts folder , ingest folder , train data path\n",
    "\t\t)\n",
    "      self.valid_test_data_store_path:str=os.path.join(\n",
    "         traning_pipline_config.artifact_dir,traning_pipline.DATA_INGESTION__DIR, traning_pipline.TEST_FILE_NAME ## artifacts folder , ingest folder , test data path\n",
    "\t\t)\n",
    "      self.invalid_traning_data_store_path:str=os.path.join(\n",
    "         traning_pipline_config.artifact_dir, traning_pipline.DATA_INGESTION__DIR, traning_pipline.TRAIN_FILE_NAME  ## artifacts folder , ingest folder , train data path\n",
    "\t\t)\n",
    "      self.invalid_test_data_store_path:str=os.path.join(\n",
    "         traning_pipline_config.artifact_dir,traning_pipline.DATA_INGESTION__DIR, traning_pipline.TEST_FILE_NAME ## artifacts folder , ingest folder , test data path\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components Output Artifacts Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DataValidationArtifact:\n",
    "    validation_status:bool\n",
    "    valid_train_path:str\n",
    "    valid_test_path:str\n",
    "    invalid_train_path:str\n",
    "    invalid_test_path:str\n",
    "    drift_report_path:str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networksecurity.logging.logger import logging\n",
    "from networksecurity.exception.exception import CustomException\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from scipy.stats import ks_2samp\n",
    "from networksecurity.utils.utills import read_yaml,write_yaml_file\n",
    "from networksecurity.logging.logger import logging\n",
    "from networksecurity.exception.exception import CustomException\n",
    "from networksecurity.entity.artifact_entity import DataIngestionArtifact\n",
    "from networksecurity.constant.traning_pipline import SCHEMA_File_DIR,SCHEMA_File_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataValidation:\n",
    "    def __init__(self, training_pipeline_config: DataValidationConfig, data_ingestion_artifacts: DataIngestionArtifact) -> None:\n",
    "        self.data_ingestion_artifacts = data_ingestion_artifacts\n",
    "        self.data_validation_config = training_pipeline_config\n",
    "        schema_file_path = os.path.join(SCHEMA_File_DIR,SCHEMA_File_NAME)\n",
    "        self.schema_config = read_yaml(schema_file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(filepath: str) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(filepath)\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def validate_num_of_cols(self, df: pd.DataFrame) -> bool:\n",
    "        try:\n",
    "            number_of_columns=len(self.schema_config['columns'])\n",
    "            \n",
    "            # checking 'Unnamed: 0' col prasent or not\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                print('Unnamed: 0 present in dataframe')\n",
    "                logging.info(f'Unnamed: 0 present in dataframe {df.columns}')\n",
    "                df.drop(columns='Unnamed: 0',axis=1,inplace=True)\n",
    "            else:\n",
    "                df\n",
    "\n",
    "            number_of_df_columns=len(df.columns)\n",
    "            logging.info(f\"Required number of columns:{number_of_columns}\")\n",
    "            logging.info(f\"Data frame has columns:{number_of_df_columns}\")\n",
    "            if number_of_df_columns==number_of_columns:\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def detect_data_drift(self, base_df: pd.DataFrame, current_df: pd.DataFrame, threshold: float = 0.05) -> bool:\n",
    "        try:\n",
    "            status = True\n",
    "            report = {}\n",
    "\n",
    "            for column in base_df.columns:\n",
    "                d1=base_df[column]\n",
    "                d2=current_df[column]\n",
    "                _, p_value=ks_2samp(d1,d2)\n",
    "                if threshold<=p_value:\n",
    "                    is_found=False\n",
    "                else:\n",
    "                    is_found=True\n",
    "                    status=False\n",
    "                report.update({column:{\n",
    "                    \"p_value\":float(p_value),\n",
    "                    \"drift_status\":is_found\n",
    "                    \n",
    "                    }})\n",
    "\n",
    "            drift_report_path = self.data_validation_config.drift_report_dir\n",
    "            os.makedirs(os.path.dirname(drift_report_path), exist_ok=True)\n",
    "            write_yaml_file(file_path=drift_report_path, content=report)\n",
    "\n",
    "            logging.info('Data drift detection completed and report generated.')\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def initiate_data_validation(self) -> DataValidationArtifact:\n",
    "        try:\n",
    "            train_file_path = self.data_ingestion_artifacts.trained_file_path\n",
    "            test_file_path = self.data_ingestion_artifacts.test_file_path\n",
    "\n",
    "            # Read training and test data\n",
    "            train_df = self.read_data(train_file_path)\n",
    "            test_df = self.read_data(test_file_path)\n",
    "            logging.info('Data read from training and test files completed.')\n",
    "\n",
    "            # Validate number of columns\n",
    "            if not self.validate_num_of_cols(df=train_df):\n",
    "                raise CustomException(\"Training data does not contain the expected columns.\")\n",
    "            if not self.validate_num_of_cols(df=test_df):\n",
    "                raise CustomException(\"Testing data does not contain the expected columns.\")\n",
    "\n",
    "            # Check for data drift\n",
    "            drift_status = self.detect_data_drift(base_df=train_df, current_df=test_df)\n",
    "            logging.info('data dreft report created successfully')\n",
    "            print('data dreft report created successfully')\n",
    "\n",
    "            # Save validated data\n",
    "            os.makedirs(os.path.dirname(self.data_validation_config.valid_traning_data_store_path), exist_ok=True)\n",
    "            train_df.to_csv(self.data_validation_config.valid_traning_data_store_path, index=False, header=True)\n",
    "            os.makedirs(os.path.dirname(self.data_validation_config.valid_test_data_store_path), exist_ok=True)\n",
    "            test_df.to_csv(self.data_validation_config.valid_test_data_store_path, index=False, header=True)\n",
    "\n",
    "            # Create DataValidationArtifact\n",
    "            data_validation_artifact = DataValidationArtifact(\n",
    "                validation_status=drift_status,\n",
    "                valid_train_path=self.data_validation_config.valid_traning_data_store_path,\n",
    "                valid_test_path=self.data_validation_config.valid_test_data_store_path,\n",
    "                invalid_train_path=None,\n",
    "                invalid_test_path=None,\n",
    "                drift_report_path=self.data_validation_config.drift_report_dir\n",
    "            )\n",
    "\n",
    "            logging.info('Data validation completed successfully.')\n",
    "            return data_validation_artifact\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networksecurity.components.data_ingestion import DataIngestion\n",
    "from networksecurity.entity.artifact_entity import DataIngestionArtifact\n",
    "from networksecurity.entity.config_entity import DataIngestionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   having_IP_Address  URL_Length  Shortining_Service  having_At_Symbol  \\\n",
      "0                 -1           1                   1                 1   \n",
      "1                  1           1                   1                 1   \n",
      "2                  1           0                   1                 1   \n",
      "3                  1           0                   1                 1   \n",
      "4                  1           0                  -1                 1   \n",
      "\n",
      "   double_slash_redirecting  Prefix_Suffix  having_Sub_Domain  SSLfinal_State  \\\n",
      "0                        -1             -1                 -1              -1   \n",
      "1                         1             -1                  0               1   \n",
      "2                         1             -1                 -1              -1   \n",
      "3                         1             -1                 -1              -1   \n",
      "4                         1             -1                  1               1   \n",
      "\n",
      "   Domain_registeration_length  Favicon  ...  popUpWidnow  Iframe  \\\n",
      "0                           -1        1  ...            1       1   \n",
      "1                           -1        1  ...            1       1   \n",
      "2                           -1        1  ...            1       1   \n",
      "3                            1        1  ...            1       1   \n",
      "4                           -1        1  ...           -1       1   \n",
      "\n",
      "   age_of_domain  DNSRecord  web_traffic  Page_Rank  Google_Index  \\\n",
      "0             -1         -1           -1         -1             1   \n",
      "1             -1         -1            0         -1             1   \n",
      "2              1         -1            1         -1             1   \n",
      "3             -1         -1            1         -1             1   \n",
      "4             -1         -1            0         -1             1   \n",
      "\n",
      "   Links_pointing_to_page  Statistical_report  Result  \n",
      "0                       1                  -1      -1  \n",
      "1                       1                   1      -1  \n",
      "2                       0                  -1      -1  \n",
      "3                      -1                   1      -1  \n",
      "4                       1                   1       1  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Unnamed: 0 present in dataframe\n",
      "Unnamed: 0 present in dataframe\n",
      "data dreft report created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "   traning_pipline_config=TraningPiplineConfig()\n",
    "   data_ingestion_config=DataIngestionConfig(traning_pipline_config=traning_pipline_config)\n",
    "   data_ingestion=DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "   data_ingestion_artifacts=data_ingestion.initiate_data_ingestion()\n",
    "   data_validation_config=DataValidationConfig(traning_pipline_config=traning_pipline_config)\n",
    "   data_validation=DataValidation(training_pipeline_config=data_validation_config,data_ingestion_artifacts=data_ingestion_artifacts)\n",
    "   data_validation.initiate_data_validation()\n",
    "except Exception as e:\n",
    "   raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
